\chapter{Introduction}
\label{chapter:intro}

Social media platforms are no longer an emerging field, they have become well established and millions of messages are exchanged daily. Services try to keep up with this trend by promoting popular content, either by number of clicks, views, favorites or other metrics. In this paper, we present a study on the clustering of messages from the Twitter platform, also known as "tweets". Inspired by the website's "Trends" which presents the most popular subjects either worldwide or in a region, the aim of our paper is to explore more of the popular subjects and cluster the conversations on more than just a keyword. The purpose of the clustering is to offer an in depth view of the conversation on a particular topic where peoples opinions may differ greatly.
\newline
The process is separated into three parts: data collection via the Twitter public API, message annotation with part-of-speech tags and message clustering. The messages provided by the website's API already provide a filtering option, you can specify keywords that you want to be part of the messages you get back. This might provide some indication of the conversation topic but getting an overview of the different conversations on the same subject is not a trivial task because messages have no obvious order.
\newline
By clustering the conversations together and offering an interface in which to explore the information, it is possible to view multiple points of view on the same subject and get more information than a popular topic might provide.

\section{Project Description}
\label{sec:proj}

In this paper we first discuss related work in the field. The popularity of the medium and the large quantity of messages have made this subject a popular topic of research. We then continue by giving a detailed explanation of the clustering process, the system architecture and the parsing and message aggregation. The next section will go into details regarding testing and the results reached and the last part will offer suggestions for further improvements and research.

\subsection{Project Scope}
\label{sub-sec:proj-scope}

We will first offer a broader explanation of the problem we wish to solve.
\newline
Twitter\footnote{https://support.twitter.com/articles/49309-using-hashtags-on-twitter} is an online micro blogging platform and social networking website. Users communicate through short 140 character messages called \"tweets\". To ease communication people use \"mentions\", the \"@\" character followed by a persons name. This is a way to involve another account into the conversation. Another feature of the conversation is the \"hashtag\", the \"\#\" sign followed by a word this is used to highlight key parts in the message, either a feeling or subject. Popular such hashtags are included in the Trending Topics. These are popular subjects automatically generated from conversations taking place worldwide or in a certain region. This is why people often times include hashtags in order to associate their message with a popular topic. Users can also favorite a tweet, and "retweet" it, meaning they share it with the people that follow them while still attributing the message to the original author. These two metrics contribute to the overall popularity of a tweet.
\newline
Twitter is an interesting platform for research because of the large number of messages exchanged daily. There are 500 million tweets sent daily by its 302 million monthly active users\footnote{https://about.twitter.com/company} with a record of 143,199 tweets per second\footnote{https://blog.twitter.com/2013/new-tweets-per-second-record-and-how}. People usually turn to Twitter during major natural events, sporting events, award ceremonies and so on. With such a high amount of information coming in every second it is almost impossible to keep track of everything that is discussed.
\newline
The "Trends" category highlights all popular topics and hashtags but exploring any one topic reveals a very large number of messages with just as more coming in every second (for popular subjects). This makes it very hard to see different points of view, different opinions on the matter. The aim of {\project}  is to offer a high level overview of the conversation, where different opinions on the same topic are grouped into different clusters. This would make it easy to identify what kind of messages you are likely to find in a cluster just by reading some of them and would allow quick understand all aspects of a developing story.
\newline
Consider the following scenario: during a global sporting event such as the football world cup which takes place over the course of several week, you want to keep track of the public opinion for your country's national team. If they are playing a game it's most likely that they are trending and a hashtag has already become popular between the people exchanging messages about the game. But how do you track the information? What if the hashtag includes both team (for example \#GERvsBRA used to track Germany - Brazil game) what if you want to see information about the first goal, or a certain player in the game. Clustering all available messages would easily reveal the ones referring to one of the teams or an individual player. This is where {\project}  will in and change the way information is being discovered.
\newline
Twitter has a powerful web platform and that is why we want to take advantage of this by allowing our project to interact with the web portal. As we present the clusters and associated messages it should be accessible to navigate between our interface and Twitter. This would allow access to the profile of the users who sent out the messages. It would give direct link to the message and therefor the context of the message would be available.

Ce nu ar trebui sa lipseasca: 
- cum ai gandit rezolvarea problemei
- care este arhitectura aplicatiei
- ce ai facut tu ca implementare
- ce metoda de testare ai folosit
- care sunt rezultatele obtinute

Chestiile astea ar trebui sa fie in jur de 30 de pagini. In plus, inainte ar trebui sa ai o introducere in care sa pui:
- descrierea problemei 1-2 pagini
- state-of-the-art (ce s-a mai facut similar) -3-4 pagini
- scurta descriere Twitter (jumatate de pagina cred ca ar fi suficient) + - ce instrumente externe ai folosit (maxim 3-4 pagini)
% my work

The scope of the project \textbf{\project} is to provide close to realtime 
clustering of conversations that take place in the online medium. My choice for
a social network is Twitter. Twitter has around 302 million active users (May 2015)
\footnote{\url{https://about.twitter.com/company}} who send 500 million tweets
each day mostly from their mobile phones. A tweet is a 140 character long message
and because of this conversations are hard to keep track of and provide little to
no context on their subject. This makes them an excellent candidate for a clustering application like \textbf{\project} which aims to provide an overview for  
conversations spanning over all the topics the user of the application provided.

% fillers

This thesis presents the \textbf{\project}.

This is an example of a footnote \footnote{\url{www.google.com}}. You can see here a reference to \labelindexref{Section}{sub-sec:proj-objectives}.

Here we have defined the CS abbreviation.\abbrev{CS}{Computer Science} and the UPB abbreviation.\abbrev{UPB}{University Politehnica of Bucharest}

The main scope of this project is to qualify xLuna for use in critical systems.


Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean aliquam lectus vel orci malesuada accumsan. Sed lacinia egestas tortor, eget tristiqu dolor congue sit amet. Curabitur ut nisl a nisi consequat mollis sit amet quis nisl. Vestibulum hendrerit velit at odio sodales pretium. Nam quis tortor sed ante varius sodales. Etiam lacus arcu, placerat sed laoreet a, facilisis sed nunc. Nam gravida fringilla ligula, eu congue lorem feugiat eu.

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean aliquam lectus vel orci malesuada accumsan. Sed lacinia egestas tortor, eget tristiqu dolor congue sit amet. Curabitur ut nisl a nisi consequat mollis sit amet quis nisl. Vestibulum hendrerit velit at odio sodales pretium. Nam quis tortor sed ante varius sodales. Etiam lacus arcu, placerat sed laoreet a, facilisis sed nunc. Nam gravida fringilla ligula, eu congue lorem feugiat eu.


\subsection{Project Objectives}
\label{sub-sec:proj-objectives}

With \textbf{\project} we want to provide close to realtime 
clustering of conversations that take place on the Twitter social social platform. As a result of its popularity the platform is generating massive amounts of messages that make it increasingly difficult to go through and read the content. We are trying to answer several questions: is it possible to manage a large number of online conversations on a particular subject, can you easily  identify the different points of view expressed, can you identify the different opinions expressed by the users of the platform.
\newline
\newline
Having answers to these questions we can easily monitor news reports, popular social events and track people's feelings about current events. It is also a test for the current technology stack available, Twitter employs hundreds of engineers to handle its infrastructure and services, we want to see if it is possible to build a system capable of scaling and handling a fraction of the traffic generated by the platform just by using open source libraries for our solution.
\newline
We want to achieve a solution that is easy to deploy and set up, that is able to connect and retrieve real time messages and that can group messages together with certain accuracy based on their topic.
\newline
To achieve this objective there are several components that go into the architecture:
\begin{itemize}
	\item Data collection --- This step involves connecting and retrieving public messages from the Twitter platform matching a particular query of interest. This is achieved via Twitter's streaming API which serves a percentage amount of all conversations. The system requires that all requests be done by authenticated clients. After registering with the service it automatically starts sending realtime messages that match your search terms. Due to the difference between the rate of processing and the rate at which messages are coming in a temporary data storage is required.
	\item Message parsing --- The tweets that have been acquired need to be parsed. By parsing we are referring to an initial processing step in which meta information is added to messages before they can enter the clustering algorithm. Due to the short nature of the messages, 10-14 words on average, the information is filled with jargon or syntactic errors in an attempt to increase the amount of information in such a small payload. In this step each word is augmented with its corresponding part of speech tag and its contribution to the overall document is computed. The importance of terms is the result of TF-IDF, a computational step in which word frequency is used to attribute a weight to each of them.
	\item Message clustering --- The result of this step will be a mapping of message ids to clusters. By cluster we are referring to a group of messages that all concern the same topic. The new messages obtained as a result of the previous step are clustered using K-Means algorithm. The algorithm requires a weight function, for this we have chosen the cosine similarity function. The algorithm works in steps, each step is an attempt to improve the overall score of a cluster (how precise is the grouping we have created). This can be quantified using the weight function we previously mentioned. This works by normalizing all the messages, a bucket is created which contains the sum of all words in all documents  and each sentence is transformed into a vector who's elements specify if a certain word form the bucket is present or not.
	\item Cluster API --- The mapping from the previous step is transformed into actual clusters of messages now that we now how to assign them together. The backend exposes the clusters of messages via an HTTP enpoint. Any number of clients can connect and have access to the information. The endpoint serves the messages in JSON format, a popular solution of web applications. This makes it easy and accessible for any number of applications to connect and consume the available data without the server having any prior knowledge of the clients it serves.
	\item Frontend --- An application that runs in the browser and connects to the endpoint described in the previous step. Using the canvas API it draws messages that belong to the same clusters closer to each other making it easy to identify them. It also displays the full message and user and allows you to explore the different clusters. This is also an easy way to verify some of our assumptions: we can expand the clusters and assert that the messages have been grouped with certain precision. The interface also links back to the author and original message making it easy to get more context or explore a certain users timeline.
\end{itemize}
This was meant to provide a high level overview of the system. We will go into further details in the Design and Implementation categories.

We have now included \labelindexref{Figure}{img:report-framework}.

\fig[scale=0.5]{src/img/reporting-framework.pdf}{img:report-framework}{Reporting Framework}


Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean aliquam lectus vel orci malesuada accumsan. Sed lacinia egestas tortor, eget tristiqu dolor congue sit amet. Curabitur ut nisl a nisi consequat mollis sit amet quis nisl. Vestibulum hendrerit velit at odio sodales pretium. Nam quis tortor sed ante varius sodales. Etiam lacus arcu, placerat sed laoreet a, facilisis sed nunc. Nam gravida fringilla ligula, eu congue lorem feugiat eu.

We can also have citations like \cite{iso-odf}.

\subsection{Related Work}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean aliquam lectus vel orci malesuada accumsan. Sed lacinia egestas tortor, eget tristiqu dolor congue sit amet. Curabitur ut nisl a nisi consequat mollis sit amet quis nisl. Vestibulum hendrerit velit at odio sodales pretium. Nam quis tortor sed ante varius sodales. Etiam lacus arcu, placerat sed laoreet a, facilisis sed nunc. Nam gravida fringilla ligula, eu congue lorem feugiat eu.


Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean aliquam lectus vel orci malesuada accumsan. Sed lacinia egestas tortor, eget tristiqu dolor congue sit amet. Curabitur ut nisl a nisi consequat mollis sit amet quis nisl. Vestibulum hendrerit velit at odio sodales pretium. Nam quis tortor sed ante varius sodales. Etiam lacus arcu, placerat sed laoreet a, facilisis sed nunc. Nam gravida fringilla ligula, eu congue lorem feugiat eu.


Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean aliquam lectus vel orci malesuada accumsan. Sed lacinia egestas tortor, eget tristiqu dolor congue sit amet. Curabitur ut nisl a nisi consequat mollis sit amet quis nisl. Vestibulum hendrerit velit at odio sodales pretium. Nam quis tortor sed ante varius sodales. Etiam lacus arcu, placerat sed laoreet a, facilisis sed nunc. Nam gravida fringilla ligula, eu congue lorem feugiat eu.

We are now discussing the \textbf{Ultimate answer to all knowledge}.
This line is particularly important it also adds an index entry for \textit{Ultimate answer to all knowledge}.\index{Ultimate answer to all knowledge}

\subsection{Demo listings}

We can also include listings like the following:

% Inline Listing example
\lstset{language=make,caption=Application Makefile,label=lst:app-make}
\begin{lstlisting}
CSRCS = app.c
SRC_DIR =..
include $(SRC_DIR)/config/application.cfg
\end{lstlisting}

Listings can also be referenced. References don't have to include chapter/table/figure numbers... so we can have hyperlinks \labelref{like this}{lst:makefile-test}.

\subsection{Tables}

We can also have tables... like \labelindexref{Table}{table:reports}.

\begin{center}
\begin{table}[htb]
  \caption{Generated reports - associated Makefile targets and scripts}
  \begin{tabular}{l*{6}{c}r}
    Generated report & Makefile target & Script \\
    \hline
    Full Test Specification & full_spec & generate_all_spec.py  \\
    Test Report & test_report & generate_report.py  \\
    Requirements Coverage & requirements_coverage &
    generate_requirements_coverage.py   \\
    API Coverage & api_coverage & generate_api_coverage.py  \\
  \end{tabular}
  \label{table:reports}
\end{table}
\end{center}
