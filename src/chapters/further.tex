\chapter{Further work}
\label{sec:furtherwork}

We have presented a solution for clustering Twitter message streams that is able to process and scale for the large amount of data delivered by its API. The project lays the foundation for several lines of work to improve overall speed and precision. 
The system is perfectly usable and but requires that each user deploys its own version of {\project}. Although the use of containers for automatic deployment via Docker reduces this task to running a script, the overhead involved with managing your own machine or environment where this can be done is a drawback. Wanting to improve on this issue but also increase the overall performance of the project, we have identified some areas where it can be improved:
\begin{itemize}
	\item As \cite{clusteralgos} suggests the clustering can be further improved by using n grams instead of matching documents just by using single terms. Single terms might skew results due to semantic ambiguity while having two or more reduces the probability of different semantical meanings. Using the StanfordNLP library which we already use in the project we are able to detect not only the part of speech corresponding to each term but also which term determines others. This information can be leveraged into producing more useful ngrams in the processing step. The new vectors associated with each document would still use term frequency to weigh the importance of the ngrams but reducing some of the semantic ambiguity should yield better results. This would also help cluster shorter phrases and messages since not all tweets use up the maximum amount of characters at their disposal and usually end up oddly clustered.
	\item Stream clustering improvements could be achieved as presented in \cite{clusteralgos}. Currently TF-IDF weight function is used to convey the importance of different terms. This method could be augmented to include the age of the documents. Current clustering issues include outliers based on new data coming into the system and at the same time old clusters becoming obsolete. To work around this problem the paper suggests taking document age into account, scaling down older documents and clusters that might not be relevant anymore and offering a higher weight to newer documents in the system. This way outliers can be promoted to clusters and slowly older clusters can be discarded.
	\item Further improving the clustering algorithm both in terms of speed and precision. Currently a bottleneck of the project is the part-of-speech library which has an average parsing speed of 1 message per second. This could be replaced with other methods for determining the relevant keywords in a message. One example is using G-test log likelihood to remove statistically insignificant terms and improve clustering precision. This solution could run on multiple threads and improve the overall performance, but tests are required to ensure the project does not suffer a loss of precision.
	\item Using a distributed solution over multiple containers or even multiple machines. The container used in deployment contains both {\project}  and the queues that hold the data. Extracting the queues would mean they can be shared between multiple instances and would be oblivious to any restarts to the server and would not lose data.
	\item Expanding the pipeline to add multiple consumers for the data, using multiple part-of-speech-taggers, and having more than one producer that retrieves data from the Twitter API. This would allow the system to retrieve more data as well as improve the time it takes to process it.
	\item Use websockets to notify {\frontend}  of new data that has been clustered. It would improve the user experience: right now when the API has new data the whole interface is redrawn including the clusters meaning that clusters may change position on the page making it harder to identify them. With a websocket implementation {\frontend}  can use event listeners and simply append new information to the page.
	\item Adding a storage layer or a caching layer in order to speed up the results. The storage could be shared between all running instances of {\project}  and provided that the same query is used clustered results could be returned instantly. The storage layer could also hold raw or parsed tweets but basic check to ensure that the data is relatively new are required. Caching would make more sense due to the fact that real time messages are expected.
\end{itemize}
We hope to explore some of these improvements ideas in the future.